# 爬虫学习笔记

## crawler_simple


### 1.导入模块

```
import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
```

- **`os`**: 提供与操作系统交互的功能，例如创建目录。
- **`re`**: 提供正则表达式功能，用于字符串处理（如清理文件名）。
- **`requests`**: 用于发送 HTTP 请求，获取网页内容。
- **`BeautifulSoup`**: 从 `bs4` 模块中导入，用于解析 HTML 和 XML 文档。
- **`urljoin`**: 从 `urllib.parse` 模块中导入，用于将相对 URL 转换为绝对 URL。



### 2.定义函数



```
def download_images(url, save_dir="images"):
    """从给定 URL 下载图像，处理相对 URL 和错误。 """
```

- 定义了一个名为

   `download_images `的函数，接收两个参数：

  - **`url`**: 目标网页的 URL。
  - **`save_dir`**: 图片保存的目录，默认值为 `"images"`。

#### 2.1 发送请求并解析HTML

```
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        img_tags = soup.find_all("img")
```

- **`requests.get(url)`**: 向目标 URL 发送 GET 请求，获取网页内容。
- **`response.raise_for_status()`**: 检查请求是否成功，如果失败会抛出异常。
- **`BeautifulSoup(response.content, "html.parser")`**: 使用 `BeautifulSoup` 解析网页内容。
- **`soup.find_all("img")`**: 查找所有 `<img>` 标签，提取图片信息。



#### 2.2检查并创建保存目录

```
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
```

- 检查指定的保存目录是否存在，如果不存在则创建该目录。



#### 2.3 遍历图片标签

```
for img in img_tags:
    img_url = img.get("src")
    if img_url:
        absolute_url = urljoin(url, img_url) # 处理相对 URL
```

#### 2.4下载图片

```
try:
    img_data = requests.get(absolute_url, stream=True)
    img_data.raise_for_status()

    if 'image' in img_data.headers.get('Content-Type', ''): #验证内容类型
        filename = os.path.basename(absolute_url)
        filename = re.sub(r'[^\w\-. ]', '_', filename)
        filepath = os.path.join(save_dir, filename)

        with open(filepath, "wb") as f:
            for chunk in img_data.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"已下载：{filename}")
    else:
        print(f"跳过非图像文件：{absolute_url}")
```

- **`requests.get(absolute_url, stream=True)`**: 向图片 URL 发送 GET 请求，使用 `stream=True` 以支持大文件下载。
- **`img_data.raise_for_status()`**: 检查请求是否成功。
- **`img_data.headers.get('Content-Type')`**: 检查响应头中的 `Content-Type`，确保是图片文件。
- **`os.path.basename(absolute_url)`**: 提取图片文件名。
- **`re.sub(r'[^\w\-. ]', '_', filename)`**: 使用正则表达式清理文件名，避免非法字符。
- **`with open(filepath, "wb") as f:`**: 打开文件以二进制写入模式保存图片。
- **`img_data.iter_content(chunk_size=8192)`**: 分块读取图片数据并写入文件。

#### 2.5异常处理

```
except requests.exceptions.RequestException as e:
    print(f"下载 {absolute_url} 失败：{e}")
```

- 捕获 `requests` 模块可能抛出的异常（如网络错误），并打印错误信息。



#### 3.调用函数

```
download_images("https://www.hippopx.com") # 将其替换为目标网址。
```

- 调用 `download_images` 函数，传入目标网站 URL（示例为 `https://www.hippopx.com`）。





> #### 总结
>
> 该脚本的主要功能是从指定的网页中爬取所有图片，并将其保存到本地目录中。它还处理了相对 URL、非图片文件过滤以及异常捕获等问题，确保程序的健壮性。

---

## crawler_better


### 1.导入模块

```
import os
import time
import random
import signal
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
```

- `os`: 用于文件和目录操作。
- `time`: 提供延迟功能，避免请求过于频繁。
- `random`: 随机选择 User-Agent 和延迟时间。
- `signal`: 处理外部信号（如 Ctrl+C），实现优雅退出。
- `requests`: 发起 HTTP 请求。
- `BeautifulSoup`: 解析 HTML 页面。
- `urljoin`: 拼接相对 URL 和基础 URL。

### 2. User-Agent 池

```
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
]
```

- **功能**: 定义一个 User-Agent 列表，模拟不同浏览器访问网页，降低被封禁的风险。

### 3.**停止标志与信号处理**

```
stop_flag = False

def signal_handler(sig, frame):
    global stop_flag
    print("收到停止信号，爬虫即将停止...")
    stop_flag = True

signal.signal(signal.SIGINT, signal_handler)
```

- 定义全局变量 `stop_flag`，用于标记是否需要停止爬虫。
- 定义 `signal_handler` 函数，捕获用户发送的中断信号（如 Ctrl+C），并设置 `stop_flag` 为 `True`。
- 使用 `signal.signal` 注册信号处理函数，确保程序可以优雅退出。



### 4. 生成随机 Headers



```
def get_random_headers(url):
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": url,
    }
    return headersxxxxxxxxxx def get_random_headers(url):    headers = {        "User-Agent": random.choice(USER_AGENTS),        "Referer": url,    }    return headers
```

- **功能**: 随机选择一个 User-Agent，并将当前 URL 设置为 Referer，构造请求头以模拟真实浏览器行为。

### 5. 添加随机延迟

```
def add_delay():
    time.sleep(random.uniform(1, 3))
```

- **功能**: 在每次请求之间随机暂停 1~3 秒，避免因请求频率过高而被目标网站屏蔽。

### 6. 下载图片的核心逻辑

```
def download_images(url, save_dir="images"):
    global stop_flag
    try:
        add_delay()
        response = requests.get(url, headers=get_random_headers(url))
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        img_tags = soup.find_all("img")

        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for img_tag in img_tags:
            if stop_flag:
                break
            img_url = img_tag.get("data-src") or img_tag.get("data-original") or img_tag.get("src")
            if img_url:
                img_url = urljoin(url, img_url)
                try:
                    add_delay()
                    srcset = img_tag.get('srcset')
                    if srcset:
                        urls = srcset.split(',')
                        high_res_url = urls[-1].split()[0]
                        img_url = urljoin(url, high_res_url)
                    img_data = requests.get(img_url, headers=get_random_headers(img_url)).content
                    img_name = os.path.basename(img_url)
                    img_path = os.path.join(save_dir, img_name)
                    with open(img_path, "wb") as f:
                        f.write(img_data)
                    print(f"Downloaded: {img_name}")
                except Exception as e:
                    print(f"Failed to download {img_url}: {e}")
        if stop_flag:
            print("爬虫已经停止")
            return

    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
```

- **获取页面内容**: 使用 `requests.get` 获取目标网页内容，并解析 HTML。
- **查找图片标签**: 使用 `BeautifulSoup` 查找所有 `<img>` 标签。
- **创建保存目录**: 如果指定的保存目录不存在，则创建该目录。
- **提取图片 URL**: 从 `<img>` 标签中提取 `data-src`、`data-original` 或 `src` 属性作为图片 URL。
- **处理高分辨率图片**: 如果存在 `srcset` 属性，优先选择其中的高分辨率图片。
- **下载图片**: 将图片保存到本地，并打印下载状态。
- **异常处理**: 捕获网络请求或文件操作中的异常，确保程序不会因单个错误而崩溃。

### 7. 主程序入口

```
if __name__ == "__main__":
    target_url = input("请输入要爬取的网页 URL: ")
    save_directory = input("请输入保存图片的目录 (默认为 'images'): ") or "images"
    download_images(target_url, save_directory)
```

- 提示用户输入目标网页 URL 和保存图片的目录。
- 调用 `download_images` 函数开始爬取和下载图片。



> #### 总结
>
> 该脚本的主要功能是从指定网页中爬取图片并保存到本地。通过以下机制提高了爬虫的稳定性和隐蔽性：
>
> 1. 随机选择 User-Agent 和 Referer。
> 2. 添加随机延迟，避免高频请求。
> 3. 支持优雅退出（Ctrl+C 停止）。
> 4. 提取高分辨率图片（如果存在 `srcset` 属性）。

---

## crawler_ultra


### 1. 模块导入

```
import os
import re
import time
import random
import signal
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
```

- `os`: 操作系统接口，用于文件和目录操作。
- `re`: 正则表达式模块，用于字符串匹配和清理。
- `time` 和 `random`: 用于控制请求间隔时间，避免被目标网站封禁。
- `signal`: 用于捕获用户中断信号（如 Ctrl+C），优雅地停止爬虫。
- `requests`: 发起 HTTP 请求的核心库。
- `pandas`: 数据处理库，用于保存下载信息到 CSV 文件。
- `BeautifulSoup`: HTML 解析库，用于提取网页内容。
- `urljoin` 和 `urlparse`: URL 操作工具，用于拼接和解析 URL。

### 2. 全局变量定义

```
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
]

PROXY_LIST = [
    {"http": "http://39.101.65.228:111", "https": "http://47.119.20.8:9098"},
]

stop_flag = False
```

- `USER_AGENTS`: 存储多个 User-Agent，模拟不同浏览器访问，降低被封禁的风险。
- `PROXY_LIST`: 存储代理 IP 列表，用于隐藏真实 IP 地址。
- `stop_flag`: 标志位，用于控制爬虫是否停止运行（例如通过 Ctrl+C 停止）。

### 3. 信号处理函数

```
def signal_handler(sig, frame):
    global stop_flag
    print("收到停止信号，爬虫即将停止...")
    stop_flag = True

signal.signal(signal.SIGINT, signal_handler)
```

- 定义了一个信号处理函数 `signal_handler`，当用户按下 Ctrl+C 时，设置 `stop_flag` 为 `True`，通知爬虫停止运行。
- 使用 `signal.signal()` 注册该函数，捕获 `SIGINT` 信号（通常是 Ctrl+C 触发的中断信号）。

### 4. 辅助函数

#### 4.1 随机生成 Headers

```
def get_random_headers(url):
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": url,
    }
    return headers
```

- 返回一个随机选择的 User-Agent，并设置 Referer 为当前 URL，模拟正常浏览器行为。

#### 4.2 获取随机代理

```
def get_random_proxy():
    if PROXY_LIST:
        return random.choice(PROXY_LIST)
    else:
        return None
```

- 如果代理列表不为空，则随机返回一个代理；否则返回 `None`。

#### 4.3 添加延迟

```
def add_delay():
    time.sleep(random.uniform(1, 3))
```

- 在每次请求之间添加随机延迟（1~3 秒），降低被封禁的风险。

#### 4.4 清理文件名

```
def sanitize_filename(filename):
    return re.sub(r'[\\/:*?"<>|]', "", filename)
```

- 去除文件名中的非法字符（如 `\`, `/`, `?`, `*` 等），确保文件名合法

#### 4.5 从 URL 获取文件名

```
def get_filename_from_url(url, content_type):
    parsed_url = urlparse(url)
    path = parsed_url.path
    filename = os.path.basename(path)
    if not filename:
        filename = "unknown"
    if "." not in filename:
        ext = content_type.split("/")[-1]
        if ext:
            filename += "." + ext
    return sanitize_filename(filename)
```

根据 URL 和 Content-Type 提取文件名：

- 如果 URL 中包含文件名，则直接使用。
- 如果没有文件名，则根据 Content-Type 自动生成扩展名（如 `.jpg`, `.png` 等）。
- 最后调用 `sanitize_filename` 清理文件名。

#### **4.6 验证代理有效性**

```
def validate_proxy(proxy):
    if not proxy:
        return True
    try:
        requests.get("https://www.hippopx.com", proxies=proxy, timeout=5)
        return True
    except Exception:
        return False
```

- 测试代理是否可用，通过尝试访问一个常用网站来验证。
- 如果代理不可用，则返回 `False`。

#### **4.7 下载资源**

```
def download_resource(url, save_dir, headers, proxies):
    try:
        add_delay()
        if proxies and validate_proxy(proxies):
            response = requests.get(url, headers=headers, proxies=proxies, stream=True, timeout=10)
        else:
            response = requests.get(url, headers=headers, stream=True, timeout=10)
        response.raise_for_status()
        content_type = response.headers.get("Content-Type", "")
        filename = get_filename_from_url(url, content_type)
        file_path = os.path.join(save_dir, filename)

        with open(file_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        print(f"Downloaded: {filename}")
        return filename, content_type
    except requests.exceptions.RequestException as e:
        print(f"Failed to download {url}: {e}")
        return None, None
```

- 下载指定 URL 的资源，支持代理和超时设置。
- 将下载的文件保存到本地目录。
- 如果下载失败，则打印错误信息并返回 `None`。

#### **4.8 提取网页文本**

```
def extract_text(soup):
    for script in soup(["script", "style"]):
        script.decompose()
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    return "\n".join(chunk for chunk in chunks if chunk)
```

- 提取网页中的纯文本内容，去除 `<script>` 和 `<style>` 标签。
- 清理多余的空格和换行符，返回格式化的文本。

### **5. 核心爬取函数**

```
def crawl_resources(url, resource_types, save_dir="downloads"):
    global stop_flag
    try:
        add_delay()
        proxy = get_random_proxy()
        headers = get_random_headers(url)
        if proxy and validate_proxy(proxy):
            response = requests.get(url, headers=headers, proxies=proxy, timeout=10)
        else:
            response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        downloaded_data = []

        if "image" in resource_types:
            img_tags = soup.find_all("img")
            os.makedirs(os.path.join(save_dir, "images"), exist_ok=True)
            for img_tag in img_tags:
                if stop_flag:
                    break
                img_url = img_tag.get("data-src") or img_tag.get("data-original") or img_tag.get("src")
                if img_url:
                    img_url = urljoin(url, img_url)
                    filename, content_type = download_resource(img_url, os.path.join(save_dir, "images"), headers, proxy)
                    if filename:
                        downloaded_data.append({"type": "image", "filename": filename, "url": img_url, "content_type": content_type})

        if "text" in resource_types:
            text = extract_text(soup)
            if text:
                filename = sanitize_filename(urlparse(url).netloc) + ".txt"
                file_path = os.path.join(save_dir, "text", filename)
                os.makedirs(os.path.join(save_dir, "text"), exist_ok=True)
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(text)
                print(f"Downloaded text: {filename}")
                downloaded_data.append({"type": "text", "filename": filename, "url": url, "content_type": "text/plain"})

        if downloaded_data:
            df = pd.DataFrame(downloaded_data)
            print(df)
            df.to_csv(os.path.join(save_dir, "downloaded_resources.csv"), index=False)

        if stop_flag:
            print("爬虫已经停止")
            return

    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
```

- 爬取指定 URL 的资源，支持图片和文本两种类型：
  - 图片：下载所有 `<img>` 标签中的图片，保存到 `images` 子目录。
  - 文本：提取网页中的纯文本内容，保存到 `text` 子目录。
- 将下载信息保存到 CSV 文件中。
- 支持通过 `stop_flag` 停止爬虫。

### **6. 主程序入口**

```
if __name__ == "__main__":
    target_url = input("请输入要爬取的网页 URL: ")
    resource_types = input("请输入要爬取的资源类型 (image, text, 多个类型用逗号分隔): ").split(",")
    resource_types = [r.strip().lower() for r in resource_types]
    save_directory = input("请输入保存资源的目录 (默认为 'downloads'): ") or "downloads"
    crawl_resources(target_url, resource_types, save_directory)
```

- 提示用户输入目标 URL、资源类型和保存目录。
- 调用 `crawl_resources` 函数开始爬取。

> ### **总结**
>
> 该脚本是一个功能完善的爬虫工具，支持以下特性：
>
> 1. **多线程防封机制**：通过随机 User-Agent 和代理 IP，降低被封禁的风险。
> 2. **灵活的资源类型**：支持图片和文本两种资源类型的爬取。
> 3. **优雅的停止机制**：支持通过 Ctrl+C 停止爬虫。
> 4. **数据保存**：将下载的资源和信息保存到本地目录和 CSV 文件中。
